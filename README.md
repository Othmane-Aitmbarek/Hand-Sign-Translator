# Hand Sign Language Translator ğŸ¤Ÿ

Plus de 70 millions de personnes sourdes dans le monde utilisent les langues des signes pour communiquer. La langue des signes leur permet dâ€™apprendre, de travailler, dâ€™accÃ©der aux services et dâ€™Ãªtre intÃ©grÃ©es dans les communautÃ©s.
Il est difficile de faire apprendre la langue des signes Ã  tout le monde afin de garantir que les personnes en situation de handicap puissent jouir de leurs droits sur un pied dâ€™Ã©galitÃ© avec les autres.
Ainsi, lâ€™objectif est de dÃ©velopper une interface homme-machine (IHM) conviviale oÃ¹ lâ€™ordinateur comprend la langue des signes amÃ©ricaine. Ce projet aidera les personnes sourdes et muettes en facilitant leur quotidien.

**Objectif** : CrÃ©er un logiciel informatique et entraÃ®ner un modÃ¨le qui prend en entrÃ©e une image en temps rÃ©el dâ€™un geste de la langue des signes amÃ©ricaine et affiche en sortie la traduction en texte.

**Scope** : Ce systÃ¨me sera bÃ©nÃ©fique Ã  la fois pour les personnes sourdes/muettes et pour celles qui ne comprennent pas la langue des signes. Il suffit de rÃ©aliser les gestes de la langue des signes, et ce systÃ¨me identifiera ce que la personne essaie de dire, puis fournira la sortie sous forme de texte.

![image]((https://www.researchgate.net/publication/328396430/figure/fig1/AS:11431281391526962@1745332210693/The-26-letters-and-10-digits-of-American-Sign-Language-ASL.tif))

## âœ¨ Features

- **Real-time hand gesture recognition** using webcam
- **High accuracy detection** with MediaPipe hand landmarks
- **Customizable gesture library** through Teachable Machine integration
- **User-friendly interface** with live video feed
- **Text translation output** for recognized gestures
- **Support for multiple hand signs** and gestures

## ğŸ› ï¸ Technologies Used

- **Python 3.10+** - Core programming language
- **OpenCV** - Computer vision and image processing
- **MediaPipe** - Hand landmark detection and tracking
- **TensorFlow** - Machine learning model inference
- **NumPy** - Numerical computations and array operations
- **Math** - Mathematical calculations for hand pose analysis
- **Teachable Machine** - Custom gesture model training

## ğŸ“‹ Prerequisites

Before running this project, make sure you have Python 3.10 or higher installed on your system.

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Othmane-Aitmbarek/Hand-Sign-Translator.git
   cd Hand-Sign-Translator
   ```

2. **Create a virtual environment** (recommended)
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install required dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“¦ Dependencies

```
opencv-python>=4.5.0
mediapipe>=0.8.0
tensorflow>=2.0.0
numpy>=1.21.0
```

Create a `requirements.txt` file with the above dependencies.

## ğŸ¯ Usage

1. **Run the main application**
   ```bash
   python test.py
   ```

2. **Position your hand** in front of the camera
3. **Make hand gestures** - the application will detect and translate them in real-time
4. **Press 'esc'** to quit the application

## ğŸ—ï¸ Project Structure

```
Hand-Sign-Translator/
â”‚
â”œâ”€â”€ Model/                  # Trained models directory
â”‚   â””â”€â”€ keras_model.h5      # Teachable Machine model
â”‚   â””â”€â”€ labels.txt          # Class labels
â”œâ”€â”€ Data/                   # Data directory
â”‚   â”œâ”€â”€ A/    
â”‚       â””â”€â”€ 0.jpg           
â”‚       â””â”€â”€ 1.jpg
â”‚   â”œâ”€â”€ B/    
â”‚       â””â”€â”€ 0.jpg         
â”‚       â””â”€â”€ 1.jpg       
â”œâ”€â”€ test.py                 # Application file
â”œâ”€â”€ camera_test.py          # Camera Testing file
â”œâ”€â”€ dataCollection.py       # Data Collection file
â””â”€â”€ tkinter_app.py          # Main application file
```

## ğŸ¤– How It Works

1. **Hand Detection**: MediaPipe identifies hand landmarks in real-time from the webcam feed
2. **Feature Extraction**: Key hand pose features are extracted and normalized
3. **Gesture Classification**: The trained Teachable Machine model classifies the gesture
4. **Translation**: Recognized gestures are translated to corresponding text/meaning
5. **Display**: Results are displayed in a user-friendly Tkinter GUI interface with confidence scores

## ğŸ“ Training Your Own Model

To train your own gesture recognition model:

1. Visit [Teachable Machine](https://teachablemachine.withgoogle.com/)
2. Choose "Image Project"
3. Create classes for different hand signs
4. Upload training images for each gesture class
5. Train the model and download it
6. Replace the model files in the `model/` directory

## ğŸ“Š Supported Gestures

Currently supported hand signs:
- A-Z Alphabet signs
- Custom gestures (can be added through training)


â­ If you found this project helpful, please give it a star!
